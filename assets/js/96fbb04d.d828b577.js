"use strict";(self.webpackChunksinnammanyo_cn=self.webpackChunksinnammanyo_cn||[]).push([[7554],{77362:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>i,default:()=>_,frontMatter:()=>c,metadata:()=>r,toc:()=>u});var s=t(87462),a=(t(67294),t(3905)),o=(t(83989),t(65488)),l=t(85162);const c={id:"opencv-dnn-ultralytics",title:"",sidebar_label:"DNN YOLO-v8-ONNX"},i=void 0,r={unversionedId:"cv/opencv/dnn/opencv-dnn-ultralytics",id:"cv/opencv/dnn/opencv-dnn-ultralytics",title:"",description:"\u4f7f\u7528 CV::DNN \u6a21\u5757\u8bfb\u53d6 ultralytics/YOLO-v8 ONNX \u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",source:"@site/docs/stack/cv/opencv/dnn/dnn-ultralytics.md",sourceDirName:"cv/opencv/dnn",slug:"/cv/opencv/dnn/opencv-dnn-ultralytics",permalink:"/stack/cv/opencv/dnn/opencv-dnn-ultralytics",draft:!1,editUrl:"https://github.com/rcxxx/sinnammanyo.cn/tree/master/docs/stack/cv/opencv/dnn/dnn-ultralytics.md",tags:[],version:"current",frontMatter:{id:"opencv-dnn-ultralytics",title:"",sidebar_label:"DNN YOLO-v8-ONNX"},sidebar:"\ud83d\udc40CV & Robot",previous:{title:"DNN YOLO-v5-ONNX",permalink:"/stack/cv/opencv/dnn/opencv-dnn-yolov5-6-0"},next:{title:"\u8f66\u9053\u7ebf\u68c0\u6d4b",permalink:"/stack/cv/opencv/demo/\u8f66\u9053\u7ebf\u68c0\u6d4b/opencv-lane-detect"}},d={},u=[{value:"\u4f7f\u7528 CV::DNN \u6a21\u5757\u8bfb\u53d6 ultralytics/YOLO-v8 ONNX \u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",id:"\u4f7f\u7528-cvdnn-\u6a21\u5757\u8bfb\u53d6-ultralyticsyolo-v8-onnx-\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",level:2},{value:"\u8fd0\u884c\u73af\u5883",id:"\u8fd0\u884c\u73af\u5883",level:3},{value:"\u83b7\u53d6 ONNX \u6a21\u578b",id:"\u83b7\u53d6-onnx-\u6a21\u578b",level:2},{value:"clone \u6e90\u7801",id:"clone-\u6e90\u7801",level:3},{value:"\u5b89\u88c5",id:"\u5b89\u88c5",level:3},{value:"\u4e0b\u8f7d\u6a21\u578b",id:"\u4e0b\u8f7d\u6a21\u578b",level:3},{value:"export \u5bfc\u51fa onnx \u6a21\u578b",id:"export-\u5bfc\u51fa-onnx-\u6a21\u578b",level:3},{value:"OpenCV-DNN \u5bfc\u5165 ONNX \u6a21\u578b",id:"opencv-dnn-\u5bfc\u5165-onnx-\u6a21\u578b",level:2},{value:"Class YoloNet()",id:"class-yolonet",level:3},{value:"\u53c2\u8003",id:"\u53c2\u8003",level:2}],p={toc:u};function _(n){let{components:e,...t}=n;return(0,a.kt)("wrapper",(0,s.Z)({},p,t,{components:e,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"\u4f7f\u7528-cvdnn-\u6a21\u5757\u8bfb\u53d6-ultralyticsyolo-v8-onnx-\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"},"\u4f7f\u7528 CV::DNN \u6a21\u5757\u8bfb\u53d6 ultralytics/YOLO-v8 ONNX \u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"),(0,a.kt)("h3",{id:"\u8fd0\u884c\u73af\u5883"},"\u8fd0\u884c\u73af\u5883"),(0,a.kt)("table",null,(0,a.kt)("thead",{parentName:"table"},(0,a.kt)("tr",{parentName:"thead"},(0,a.kt)("th",{parentName:"tr",align:null},"name"),(0,a.kt)("th",{parentName:"tr",align:"center"},"version"))),(0,a.kt)("tbody",{parentName:"table"},(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"System"),(0,a.kt)("td",{parentName:"tr",align:"center"},(0,a.kt)("strong",{parentName:"td"},(0,a.kt)("a",{parentName:"strong",href:"https://releases.ubuntu.com/20.04/"},"Ubuntu 20.04")))),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"CMake"),(0,a.kt)("td",{parentName:"tr",align:"center"},(0,a.kt)("strong",{parentName:"td"},(0,a.kt)("a",{parentName:"strong",href:"https://cmake.org/"},"3.24")))),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"OpenCV"),(0,a.kt)("td",{parentName:"tr",align:"center"},(0,a.kt)("strong",{parentName:"td"},(0,a.kt)("a",{parentName:"strong",href:"https://github.com/opencv/opencv/releases/tag/4.7.0"},"4.7.0")))),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"Python"),(0,a.kt)("td",{parentName:"tr",align:"center"},(0,a.kt)("strong",{parentName:"td"},(0,a.kt)("a",{parentName:"strong",href:"https://www.python.org/downloads/release/python-380/"},"3.8.0")))),(0,a.kt)("tr",{parentName:"tbody"},(0,a.kt)("td",{parentName:"tr",align:null},"YOLOv8"),(0,a.kt)("td",{parentName:"tr",align:"center"},(0,a.kt)("strong",{parentName:"td"},(0,a.kt)("a",{parentName:"strong",href:"https://github.com/ultralytics/ultralytics"},"ultralytics")))))),(0,a.kt)("h2",{id:"\u83b7\u53d6-onnx-\u6a21\u578b"},"\u83b7\u53d6 ONNX \u6a21\u578b"),(0,a.kt)("h3",{id:"clone-\u6e90\u7801"},"clone \u6e90\u7801"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/ultralytics/ultralytics.git\n")),(0,a.kt)("h3",{id:"\u5b89\u88c5"},"\u5b89\u88c5"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd ultralytics/\npip install -r requirements.txt\npip install ultralytics\n")),(0,a.kt)("h3",{id:"\u4e0b\u8f7d\u6a21\u578b"},"\u4e0b\u8f7d\u6a21\u578b"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},'yolo predict model=yolov8s.pt source="https://ultralytics.com/images/bus.jpg"\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"model={}")," \u586b\u5165\u6240\u9700\u8981\u7684\u6a21\u578b\uff0c\u5982\u679c\u672c\u5730\u6ca1\u6709\u5219\u4f1a\u4ece\u4ed3\u5e93\u4e0b\u8f7d")),(0,a.kt)("h3",{id:"export-\u5bfc\u51fa-onnx-\u6a21\u578b"},"export \u5bfc\u51fa onnx \u6a21\u578b"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u5b89\u88c5 onnx")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"pip install onnx\npip install onnxsim\n")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u5bfc\u51fa\u6a21\u578b")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"yolo export \\\nmodel=yolov8s.pt \\\nimgsz=[640,640] \\\nformat=onnx \\\nopset=12\n")),(0,a.kt)("h2",{id:"opencv-dnn-\u5bfc\u5165-onnx-\u6a21\u578b"},"OpenCV-DNN \u5bfc\u5165 ONNX \u6a21\u578b"),(0,a.kt)("h3",{id:"class-yolonet"},"Class YoloNet()"),(0,a.kt)(o.Z,{defaultValue:"cc",values:[{label:"c++",value:"cc"},{label:"python",value:"py"}],mdxType:"Tabs"},(0,a.kt)(l.Z,{value:"cc",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-cpp",metastring:'title="yolov8_onnx.hpp"',title:'"yolov8_onnx.hpp"'},"#ifndef YOLOV8_ONNX_OPENCV_YOLOV_8_ONNX_HPP\n#define YOLOV8_ONNX_OPENCV_YOLOV_8_ONNX_HPP\n\n#include <fstream>\n#include <vector>\n#include <string>\n#include <random>\n\n#include <opencv2/imgproc.hpp>\n#include <opencv2/opencv.hpp>\n#include <opencv2/dnn.hpp>\n\nnamespace yolov8_onnx {\n    struct Detection\n    {\n        int class_id{0};\n        std::string className{};\n        float confidence{0.0};\n        cv::Scalar color{};\n        cv::Rect box{};\n    };\n\n    class Net\n    {\n    public:\n        explicit Net(const std::string &onnx_model_path, const std::string &classes_txt_file_path,\n                     const cv::Size &model_input_shape = {640, 640}, const bool &is_cuda = false);\n\n        ~Net()= default;\n\n        std::vector<yolov8_onnx::Detection> detect(cv::Mat &src,\n                                              float _score_threshold = 0.45,\n                                              float _NMS_threshold = 0.5,\n                                              float _confidence_threshold = 0.4);\n        inline std::vector<std::string> classList(){\n            return this->class_list_;\n        }\n\n    private:\n        cv::dnn::Net net;\n        std::vector<std::string> class_list_;\n\n        cv::Size2f model_shape_{};\n\n        static cv::Mat format_img(const cv::Mat &_src);\n    };\n};\n\n#endif //YOLOV8_ONNX_OPENCV_YOLOV_8_ONNX_HPP\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-cpp",metastring:'title="yolov8_onnx.cpp"',title:'"yolov8_onnx.cpp"'},'#include "yolov_8_onnx.hpp"\n\nnamespace yolov8_onnx {\n\n    Net::Net(const std::string &onnx_model_path, const std::string &classes_txt_file_path,\n             const cv::Size &model_input_shape, const bool &is_cuda) {\n        // load models\n        this->net = cv::dnn::readNetFromONNX(onnx_model_path);\n        if (is_cuda)\n        {\n            std::cout << "\\nRunning on CUDA" << std::endl;\n            net.setPreferableBackend(cv::dnn::DNN_BACKEND_CUDA);\n            net.setPreferableTarget(cv::dnn::DNN_TARGET_CUDA);\n        }\n        else\n        {\n            std::cout << "\\nRunning on CPU" << std::endl;\n            net.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV);\n            net.setPreferableTarget(cv::dnn::DNN_TARGET_CPU);\n        }\n\n        // load class_list\n        std::ifstream ifs(classes_txt_file_path);\n        if (ifs.is_open())\n        {\n            std::string line;\n            while (getline(ifs, line))\n            {\n                this->class_list_.push_back(line);\n            }\n        }\n\n        this->model_shape_ = model_input_shape;\n    }\n\n    std::vector<yolov8_onnx::Detection>\n    Net::detect(cv::Mat &src,\n                float _score_threshold,\n                float _NMS_threshold,\n                float _confidence_threshold) {\n        // format image\n        cv::Mat input = format_img(src);\n\n        cv::Mat blob;\n        cv::dnn::blobFromImage(input, blob, 1.0/255.0, this->model_shape_, cv::Scalar(), true, false);\n        net.setInput(blob);\n\n        std::vector<cv::Mat> outputs;\n        net.forward(outputs, net.getUnconnectedOutLayersNames());\n\n        // yolo_v8 has an output of shape (batchSize, 84,  8400) (Num classes + box[x,y,w,h])\n        int rows = outputs[0].size[2];\n        int dimensions = outputs[0].size[1];\n\n        outputs[0] = outputs[0].reshape(1, dimensions);\n        cv::transpose(outputs[0], outputs[0]);\n\n        auto *data = (float *)outputs[0].data;\n\n        float x_factor = static_cast<float >(input.cols) / this->model_shape_.width;\n        float y_factor = static_cast<float >(input.rows) / this->model_shape_.height;\n\n        std::vector<int> class_ids;\n        std::vector<float> confidences;\n        std::vector<cv::Rect> boxes;\n\n        for (int i = 0; i < rows; ++i)\n        {\n            float *classes_scores = data+4;\n\n            cv::Mat scores(1, static_cast<int>(this->class_list_.size()), CV_32FC1, classes_scores);\n            cv::Point class_id;\n            double max_class_score;\n\n            minMaxLoc(scores, 0, &max_class_score, 0, &class_id);\n            if (max_class_score > _score_threshold){\n                confidences.push_back(static_cast<float>(max_class_score));\n                class_ids.push_back(class_id.x);\n\n                float x = data[0];\n                float y = data[1];\n                float w = data[2];\n                float h = data[3];\n\n                int left = static_cast<int>((x - 0.5 * w) * x_factor);\n                int top = static_cast<int>(( y - 0.5 * h) * y_factor);\n\n                int width = static_cast<int>(w * x_factor);\n                int height = static_cast<int>(h * y_factor);\n\n                boxes.emplace_back(left, top, width, height);\n            }\n            data += dimensions;\n        }\n\n        std::vector<int> nms_result;\n        cv::dnn::NMSBoxes(boxes, confidences, _score_threshold, _NMS_threshold, nms_result);\n\n        std::vector<yolov8_onnx::Detection> detections{};\n        for(int idx : nms_result){\n            yolov8_onnx::Detection result;\n            result.class_id = class_ids[idx];\n            result.confidence = confidences[idx];\n\n            std::random_device rd;\n            std::mt19937 gen(rd());\n            std::uniform_int_distribution<int> dis(100, 255);\n            result.color = cv::Scalar(dis(gen),\n                                      dis(gen),\n                                      dis(gen));\n\n            result.className = class_list_[result.class_id];\n            result.box = boxes[idx];\n\n            detections.push_back(result);\n        }\n\n        return detections;\n    }\n\n    cv::Mat Net::format_img(const cv::Mat &_src) {\n        int format_size = MAX(_src.cols, _src.rows);\n\n        cv::Mat dst = cv::Mat::zeros(cv::Size(format_size, format_size), CV_8UC3);\n        _src.copyTo(dst(cv::Rect(0, 0, _src.cols, _src.rows)));\n\n        return dst;\n    }\n};\n')),(0,a.kt)("blockquote",null,(0,a.kt)("ul",{parentName:"blockquote"},(0,a.kt)("li",{parentName:"ul"},"v8 \u4e0e v5 \u5728 output \u7ed3\u6784\u4e0a\u6709\u533a\u522b\n// yolov5 has an output of shape (batchSize, 25200, 85) (Num classes + box","[x,y,w,h]"," + confidence","[c]",")\n// yolov8 has an output of shape (batchSize, 84,  8400) (Num classes + box","[x,y,w,h]",")"))),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-cpp",metastring:'title="main.cpp"',title:'"main.cpp"'},'#include <iostream>\n#include <opencv2/opencv.hpp>\n\n#include "yolov8/yolov_8_onnx.hpp"\n\nint main() {\n    std::string model_path = "../models/yolov8s.onnx";\n    std::string classes_path = "../models/classes.txt";\n    std::string img_path = "../img/bus.jpg";\n\n    yolov8_onnx::Net yolo(model_path, classes_path);\n\n    cv::Mat src_img = cv::imread(img_path);\n\n    // YOLO detect\n    std::vector<yolov8_onnx::Detection> results = yolo.detect(src_img);\n    for (const auto &idx : results) {\n        auto bbox = idx.box;\n        auto classId = idx.class_id;\n        cv::rectangle(src_img, bbox, idx.color, 2);\n        cv::rectangle(src_img, cv::Point(bbox.x, bbox.y + 14), cv::Point(bbox.x + bbox.width, bbox.y), idx.color, cv::FILLED);\n        cv::putText(src_img, yolo.classList()[classId], cv::Point(bbox.x, bbox.y + 10), cv::FONT_HERSHEY_SIMPLEX, 0.6, cv::Scalar(0, 0, 0));\n    }\n\n    cv::imshow("src_img", src_img);\n    cv::waitKey(0);\n\n    return 0;\n}\n')),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-cmake",metastring:'title="CMakeLists.txt"',title:'"CMakeLists.txt"'},"cmake_minimum_required(VERSION 3.24)\nproject(yolov8_onnx_opencv)\n\nset(CMAKE_CXX_STANDARD 20)\nset(CMAKE_CXX_STANDARD_REQUIRED TRUE)\n\n# Add include dir\ninclude_directories(\n        ${PROJECT_SOURCE_DIR}/modules\n)\n\nadd_executable(yolov8_onnx_opencv src/main.cpp)\n\n# Dependence lib\n## Find and add opencv as required\nfind_package(OpenCV REQUIRED)\ninclude_directories(${OpenCV_INCLUDE_DIRS})\nlink_libraries(${OpenCV_LIBS})\n\n# Add Subdirectories\nadd_subdirectory(${PROJECT_SOURCE_DIR}/modules/yolov8)\n\n# Target\ntarget_link_libraries(yolov8_onnx_opencv\n        ${OpenCV_LIBS}\n        yolov8)\n")),(0,a.kt)("p",null,"\u8f93\u51fa\u7ed3\u679c\u5982\u4e0b"),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://pictures-1304295136.cos.ap-guangzhou.myqcloud.com/screenshot/ubuntu/yolov8/yolov8-result.png",alt:null}))),(0,a.kt)(l.Z,{value:"py",mdxType:"TabItem"},(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="yolov8_onnx.py"',title:'"yolov8_onnx.py"'},"import cv2\nimport numpy as np\n\nfrom ultralytics.yolo.utils import yaml_load\nfrom ultralytics.yolo.utils.checks import check_yaml\n\nclass Model:\n    def __init__(self, _model_path, _size=(640,640)):\n        self.model = cv2.dnn.readNetFromONNX(_model_path)\n        self.CLASSES = yaml_load(check_yaml('coco128.yaml'))['names']\n        self.size = _size\n        self.colors = np.random.uniform(0, 255, size=(len(self.CLASSES), 3))\n\n    def draw_bounding_box(self, _img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n        label = f'{self.CLASSES[class_id]} ({confidence:.2f})'\n        color = self.colors[class_id]\n        cv2.rectangle(_img, (x, y), (x_plus_w, y_plus_h), color, 2)\n        cv2.putText(_img, label, (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    def det(self, _img ,_score_th=0.45, _NMS_th = 0.5):\n        [height, width, _] = _img.shape\n        length = max(height, width)\n        img = np.zeros((length, length, 3), np.uint8)\n        img[0:height, 0:width] = _img\n        scale = length / self.size[0]\n        blob = cv2.dnn.blobFromImage(img, scalefactor= 1/255, size=self.size)\n        self.model.setInput(blob)\n        outputs = self.model.forward()\n\n        outputs = np.array([cv2.transpose(outputs[0])])\n        rows = outputs.shape[1]\n\n        boxes = []\n        scores = []\n        class_ids = []\n\n        for i in range(rows):\n            classes_scores = outputs[0][i][4:]\n            (min_score, max_score, min_class_loc, (x, max_class_index)) = cv2.minMaxLoc(classes_scores)\n            if max_score >= _score_th:\n                box = [outputs[0][i][0] - (0.5 * outputs[0][i][2]),\n                       outputs[0][i][1] - (0.5 * outputs[0][i][3]),\n                       outputs[0][i][2],\n                       outputs[0][i][3]]\n                boxes.append(box)\n                scores.append(max_score)\n                class_ids.append(max_class_index)\n\n        result_boxes = cv2.dnn.NMSBoxes(boxes, scores, _score_th, _NMS_th, 0.5)\n        detections = []\n        for i in range(len(result_boxes)):\n            index = result_boxes[i]\n            box = boxes[index]\n            detection = {\n                'class_id': class_ids[index],\n                'class_name': self.CLASSES[class_ids[index]],\n                'confidence': scores[index],\n                'box': box,\n                'scale': scale}\n            detections.append(detection)\n            self.draw_bounding_box(_img, class_ids[index], scores[index],\n                                   round(box[0] * scale),\n                                   round(box[1] * scale),\n                                   round((box[0] + box[2]) * scale),\n                                   round((box[1] + box[3]) * scale))\n\n        return detections\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="main.py"',title:'"main.py"'},"import cv2\nimport numpy as np\n\nfrom yolov8_onnx import Model\n\ndef main():\n    net = Model('yolov8s-480x.onnx', _size=(640, 640))\n    src_img = cv2.imread('bus.jpg')\n\n    detections = net.det(src_img)\n\n    for i in range(len(detections)):\n        detection = detections[i]\n        net.draw_bounding_box(src_img,\n                              detection['class_id'],\n                              detection['confidence'],\n                              round(detection['box'][0] * detection['scale']),\n                              round(detection['box'][1] * detection['scale']),\n                              round((detection['box'][0] + detection['box'][2]) * detection['scale']),\n                              round((detection['box'][1] + detection['box'][3]) * detection['scale'])\n                              )\n\n    cv2.imshow('image', src_img)\n    cv2.waitKey(0)\n\nif __name__ == '__main__':\n    main()\n    cv2.destroyAllWindows()\n\n")),(0,a.kt)("p",null,"\u8f93\u51fa\u7ed3\u679c\u5982\u4e0b"),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://pictures-1304295136.cos.ap-guangzhou.myqcloud.com/screenshot/ubuntu/yolov8/yolov8-result-py.png",alt:null})))),(0,a.kt)("h2",{id:"\u53c2\u8003"},"\u53c2\u8003"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("a",{parentName:"strong",href:"https://github.com/ultralytics/ultralytics"},"ultralytics/ultralytics"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("a",{parentName:"strong",href:"https://github.com/ultralytics/ultralytics/tree/main/examples/YOLOv8-CPP-Inference"},"ultralytics/examples/YOLOv8-CPP-Inference")))))}_.isMDXComponent=!0},85162:(n,e,t)=>{t.d(e,{Z:()=>l});var s=t(67294),a=t(86010);const o="tabItem_Ymn6";function l(n){let{children:e,hidden:t,className:l}=n;return s.createElement("div",{role:"tabpanel",className:(0,a.Z)(o,l),hidden:t},e)}},65488:(n,e,t)=>{t.d(e,{Z:()=>_});var s=t(87462),a=t(67294),o=t(86010),l=t(72389),c=t(67392),i=t(7094),r=t(12466);const d="tabList__CuJ",u="tabItem_LNqP";function p(n){const{lazy:e,block:t,defaultValue:l,values:p,groupId:_,className:m}=n,v=a.Children.map(n.children,(n=>{if((0,a.isValidElement)(n)&&"value"in n.props)return n;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof n.type?n.type:n.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})),h=p??v.map((n=>{let{props:{value:e,label:t,attributes:s}}=n;return{value:e,label:t,attributes:s}})),b=(0,c.l)(h,((n,e)=>n.value===e.value));if(b.length>0)throw new Error(`Docusaurus error: Duplicate values "${b.map((n=>n.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`);const N=null===l?l:l??v.find((n=>n.props.default))?.props.value??v[0].props.value;if(null!==N&&!h.some((n=>n.value===N)))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${N}" but none of its children has the corresponding value. Available values are: ${h.map((n=>n.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);const{tabGroupChoices:g,setTabGroupChoices:x}=(0,i.U)(),[f,y]=(0,a.useState)(N),k=[],{blockElementScrollPositionUntilNextRender:O}=(0,r.o5)();if(null!=_){const n=g[_];null!=n&&n!==f&&h.some((e=>e.value===n))&&y(n)}const C=n=>{const e=n.currentTarget,t=k.indexOf(e),s=h[t].value;s!==f&&(O(e),y(s),null!=_&&x(_,String(s)))},w=n=>{let e=null;switch(n.key){case"Enter":C(n);break;case"ArrowRight":{const t=k.indexOf(n.currentTarget)+1;e=k[t]??k[0];break}case"ArrowLeft":{const t=k.indexOf(n.currentTarget)-1;e=k[t]??k[k.length-1];break}}e?.focus()};return a.createElement("div",{className:(0,o.Z)("tabs-container",d)},a.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.Z)("tabs",{"tabs--block":t},m)},h.map((n=>{let{value:e,label:t,attributes:l}=n;return a.createElement("li",(0,s.Z)({role:"tab",tabIndex:f===e?0:-1,"aria-selected":f===e,key:e,ref:n=>k.push(n),onKeyDown:w,onClick:C},l,{className:(0,o.Z)("tabs__item",u,l?.className,{"tabs__item--active":f===e})}),t??e)}))),e?(0,a.cloneElement)(v.filter((n=>n.props.value===f))[0],{className:"margin-top--md"}):a.createElement("div",{className:"margin-top--md"},v.map(((n,e)=>(0,a.cloneElement)(n,{key:e,hidden:n.props.value!==f})))))}function _(n){const e=(0,l.Z)();return a.createElement(p,(0,s.Z)({key:String(e)},n))}}}]);